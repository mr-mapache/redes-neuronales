{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes neuronales Feed Forward\n",
    "\n",
    "Las redes \"feed forward\" o $\\text{FFN}$ de ahora en adelante, son tipo de redes neuronales en las que los datos entran por la capa de entrada y se transmiten en una única dirección hacia la capa de salida. El tipo de red neuronal mas común del tipo $\\text{FFN}$ son los perceptrones multicapa, que consisten en una serie de capas de neuronas, donde cada neurona de una capa está conectada con todas las neuronas de la capa siguiente.\n",
    "\n",
    "En este trabajo estaremos comparando el rendimiento de redes $\\text{FFN}$ del tipo autoencoder, ya que queremos evaluarlas en conjunto con los metodos de normalización disponibles, ya que es asi como se utilizan en los transformers para aprender a transformar las secuencias de una representacion en el espacio de embeddings en otra entre las capas de la red. \n",
    "\n",
    "Tambien compararemos el rendimiento de estas redes en tareas de clasificación entrenandolos con una cabeza clasificadora en la capa de salida y una capa de normalización entre el autoencoder y la cabeza clasificadora, tal como se hacen en los $\\text{ViT}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos a comparar\n",
    "\n",
    "Compararemos los siguientes modelos:\n",
    "\n",
    "1. Multi layer perceptron $\\text{MLP}$ clásico, con dropout y bias en la capa encoder.\n",
    "2. Gated linear units $\\text{GLU}$ descrito en el paper \"GLU Variants Improve Transformer\" utilizado en $\\text{Llama3}$.\n",
    "3. Modelo basado en $\\text{DropConnect}$ descrito en el paper \"Regularization of Neural Networks using DropConnect\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elección del tamaño de la capa oculta.\n",
    "\n",
    "Para una comparación justa, elegiremos los tamaños de las capas ocultas para las redes de forma tal que todas tengan el mismo numero de parametros. La memoria vram es un recurso limitado y es el principal cuello de botella en el entrenamiento de redes neuronales, y por eso el numero de parametros debe ser el factor determinante en la elección del tamaño de las capas ocultas. \n",
    "\n",
    "Sean:\n",
    "\n",
    "- $d$ la dimensión del modelo.\n",
    "- $h$ el tamaño de la capa oculta de la red tipo $\\text{MLP}$.\n",
    "- $h'$ el tamaño de la capa oculta de la red tipo $\\text{GLU}$.\n",
    "\n",
    "El numero de parámetros de la red tipo $\\text{MLP}$ sera de $dh + hd = 2dh $, sin bias y $dh + hd + h = (2d + 1)h$ con bias. Notemos que ignoramos el termino de bias en la ultima capa debido a que este es incompatible con los metodos normalización utilizados actualmente.\n",
    "\n",
    "El numero de parámetros de la red tipo $\\text{GLU}$ sera de $3dh'$ sin bias y $3dh' + h' = (3d + 1)h'$ con bias en la capa de entrada, de modo que para que ambas redes tengan el mismo numero de parametros, debemos elegir $h$ y $h'$ de forma tal que:\n",
    "\n",
    "$$2dh = 3dh' \\Rightarrow h = \\frac{3}{2}h' \\quad \\text{sin bias}$$\n",
    "$$(2d + 1)h = (3d + 1)h' \\Rightarrow h = \\frac{3d + 1}{2d + 1}h' = \\frac{3}{2} (1 - \\frac{1}{6d + 3}) h' \\approx \\frac{3}{2}h' \\quad \\text{con bias}$$\n",
    "\n",
    "De modo que la red del tipo $\\text{MLP}$ debe tener $\\frac{3}{2}$ veces el tamaño de la capa oculta de la red del tipo $\\text{GLU}$.\n",
    "\n",
    "(Vease \"Batch Normalization: Accelerating Deep Network Training by\n",
    "Reducing Internal Covariate Shift\", \"Understanding the Disharmony between Dropout and Batch Normalization by\n",
    "Variance Shift\", la misma explicación aplica para los otros métodos de normalización)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
