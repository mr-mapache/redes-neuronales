{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes neuronales Feed Forward\n",
    "\n",
    "Las redes \"feed forward\" o $\\text{FFN}$ de ahora en adelante, son tipo de redes neuronales en las que los datos entran por la capa de entrada y se transmiten en una única dirección hacia la capa de salida. El tipo de red neuronal mas común del tipo $\\text{FFN}$ son los perceptrones multicapa, que consisten en una serie de capas de neuronas, donde cada neurona de una capa está conectada con todas las neuronas de la capa siguiente.\n",
    "\n",
    "En este trabajo estaremos comparando el rendimiento de redes $\\text{FFN}$ del tipo autoencoder, ya que es asi como se utilizan en los transformers para aprender a transformar las secuencias de una representacion en el espacio de embeddings en otra entre las capas de la red, y a que queremos evaluarlas en conjunto con los metodos de normalización disponibles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elección del tamaño de la capa oculta.\n",
    "\n",
    "Sean:\n",
    "\n",
    "- $d$ la dimensión del modelo.\n",
    "- $h$ el tamaño de la capa oculta de la red tipo $\\text{MLP}$.\n",
    "- $h'$ el tamaño de la capa oculta de la red tipo $\\text{GLU}$.\n",
    "\n",
    "El numero de parámetros de la red tipo $\\text{MLP}$ sera de $dh + hd = 2dh $, sin bias y $dh + hd + h = (2d + 1)h$ con bias. Notemos que ignoramos el termino de bias en la ultima capa debido a que este es incompatible con los metodos normalización utilizados actualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module, Linear, Sequential\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Dropout\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, model_dimension: int, hidden_dimension: int, bias: bool = True, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.input_layer = Linear(model_dimension, hidden_dimension, bias=bias)\n",
    "        self.activation = ReLU()\n",
    "        self.dropout = Dropout(p)\n",
    "        self.output_layer = Linear(hidden_dimension, model_dimension, bias=False)\n",
    "       \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        output = self.activation(self.input_layer(input))\n",
    "        output = self.dropout(output)\n",
    "        return self.output_layer(output)\n",
    "\n",
    "class GLU(Module):\n",
    "    def __init__(self, model_dimension: int, hidden_dimension: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        if not hidden_dimension % 3 == 0:\n",
    "            print(\"Warning: hidden_dimension should be a multiple of 3 for having the same number of parameters as a regular MLP\")\n",
    "        self.hidden_dimension = (hidden_dimension * 2) // 3\n",
    "        self.gate_dimension = hidden_dimension \n",
    "\n",
    "        self.activation = ReLU()\n",
    "        self.input_layer = Linear(model_dimension, self.hidden_dimension, bias=bias)\n",
    "        self.output_layer = Linear(self.hidden_dimension, hidden_dimension, bias=False)\n",
    "        self.gate_layer = Linear(model_dimension, self.hidden_dimension, bias=False)\n",
    "       \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        output = self.activation(self.input_layer(input)) * self.gate_layer(input)\n",
    "        return self.output_layer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE_S = 384\n",
    "HIDDEN_SIZE_M = 576 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
